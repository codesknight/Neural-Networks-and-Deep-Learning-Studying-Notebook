#### 原文翻译

##### 第4章 用可视化方法证明神经网络可以计算任何函数

关于神经网络最引人注目的事实之一就是它们可以计算任何函数。也就是说，假设有人给了你一些复杂的，扭曲的函数$f(x)$:
![alt text](image.png)
不管是什么函数都保证有一个神经网络对于每一个可能的输入$x$,都会从网络中得到一个输出值$f(x)$（或近似值）,例如：
![alt text](image-1.png)
即使这个函数有很多输入和输出，这个结果也仍然成立,$f(x)=f(x_1,x_2,...,x_m)$。例如，下面的神经网络计算的就是带有3个输入和2个输出的函数。
![alt text](image-2.png)
这个结果告诉我们神经网络具有一种普遍性。不管我们想要计算什么样的函数，我们都会想到有一个神经网络可以帮助我们计算出这个函数（或者说可以近似得到这个函数）。

更重要的是，即使我们限制我们的网络在输入和输出神经元之间只有一个中间层——一个所谓的单一隐藏层，这个普遍性定理仍然成立。因此，即使是非常简单的网络架构也可以非常强大。

普遍性定理为使用神经网络的人所熟知。但是为什么这却没有得到大众的广泛的理解。大多数的解释是相当技术性的。例如，George Cybenko (1989)的一篇原始论文证明了一个 S 函数的叠加逼近的结果。这个结果在当时是非常普遍的，而且几个小组证明了密切相关的结果[1]。Cybenko 的论文对这方面的大部分工作进行了有益的讨论。另一篇重要的早期论文是由 Kurt Hornik，Maxwell Stinchcombe 和 Halbert White (1989)提出的多层前馈网络是通用逼近器。本文利用 Stone-Weierstrass 定理得到了类似的结果。运用了哈恩-巴纳赫定理里斯表示定理和一些傅立叶变换家族中的关系。如果你是一个数学家，这个理论并不难理解，但是对于大多数人来说就不那么容易了。这是一个遗憾，因为普遍性的根本原因是简单而美丽的。

在这一章中，我给出了普遍性定理的一个简单而直观的解释。我们将一步一步地通过基本的想法。你会明白为什么神经网络可以计算任何函数。您将理解这个结果的一些局限性。你就会明白这个结果与深层神经网络之间的关系。

为了跟随本章的内容，你不需要阅读本书的前几章。相反，这一章的结构是一个独立的文章享受。只要你对神经网络有一点点基本的了解，你就应该能够理解这个解释。不过，我会偶尔提供早期材料的链接，以帮助填补您的知识空白。

普遍性定理在计算机科学中是司空见惯的，以至于我们有时会忘记它们是多么惊人。但是值得提醒自己的是: 计算任意函数的能力确实非凡。几乎你能想象到的任何过程都可以被认为是函数计算。思考一下根据一小段音乐样本来命名一段音乐的问题就可以认为是计算一个函数。或者考虑一下把中文文本翻译成英文的问题，同样，这可以被认为是计算一个函数[2]。或者考虑一下使用 mp4电影文件并生成对电影情节的描述以及对表演质量的讨论，这可以被看作是一种函数计算[3]。普遍性意味着,原则上，神经网络可以做所有这些事情和更多的事情。

当然，仅仅因为我们知道存在一个神经网络可以将中文文本翻译成英文，并不意味着我们有很好的技术来构建甚至识别这样一个网络。这种局限性也适用于布尔电路等模型的传统普适性定理。但是，正如我们在前面的书中看到的，神经网络有强大的学习功能算法。这种学习算法 + 通用性的组合是一种有吸引力的组合。到目前为止，这本书的重点是学习算法。在本章中，我们将重点讨论普遍性及其意义。

##### 两个注意事项
在解释为什么普遍性定理是正确的之前，我想提到两个非正式陈述“神经网络可以计算任何函数”的注意事项。

首先，这并不意味着网络可以用来精确地计算任何函数。相反，我们可以得到一个尽可能好的近似值。通过增加隐藏神经元的数量，我们可以提高近似值。例如，前面我举了一个用三个隐藏的神经元的网络计算函数 $f(x)$的例子。对于大多数函数来说，只有使用三个隐藏的神经元才能得到低质量的近似值。通过增加隐藏神经元的数量(比如说，增加到5个) ，我们通常可以得到一个更好的近似值:
![alt text](image-3.png)
通过进一步增加隐藏神经元的数量，我们可以做得更好。

为了使这个陈述更精确，假设我们给出了一个函数$f(x)$我们希望能够在一定的精度范围内计算 ε > 0。通过使用足够的隐藏神经元，我们总是可以找到一个神经网络的输出$g(x)$满足$|g(x)-f(x)|< ε$,对于所有的输入$x$。换句话说，对于每一种可能的输入，这种近似将在预期的精度范围内。

第二个注意事项是，这一类需要近似得到的函数必须是连续函数。如果一个函数是不连续的，例如，跳跃函数，那么通常不可能用神经网络来近似。这并不奇怪，因为我们的神经网络计算的就是输入的连续函数。然而，即使我们真正想要计算的函数是不连续的，通常情况下，连续的近似其实也足够好了。如果是这样的话，我们就可以用神经网络来近似了。在实践中，这通常不是一个重要的限制。总而言之，对于普遍性定理的一个更精确的描述是，具有单个隐层的神经网络可以用来逼近任意连续函数，达到任意期望的精度。在本章中，我们实际上将证明这个结果的一个稍微弱一点的版本，使用两个隐藏层而不是一个。在这些问题中，我将简要地概述如何通过一些调整，使解释适合于给出一个只使用一个隐藏层的证明。

##### 只有一个输入和一个输出的普遍性定理
为了理解普适性定理为什么成立，让我们从构建一个能近似得到一个函数的只有一个输入和一个输出的神经网络开始：
![alt text](image-4.png)
这就是普遍性问题的核心。一旦我们理解了这个特殊情况，实际上就很容易扩展到具有多个输入和多个输出的函数。

为了深入理解如何构建一个计算$f(x)$的网络，让我们从一个仅包含一个隐藏层、两个隐藏神经元和一个输出层包含一个输出神经元的网络开始：
![alt text](image-5.png)
为了了解网络中的组件是如何工作的，让我们把注意力集中在顶部隐藏的神经元上。在下图中，单击权重$w$，并拖动鼠标向右一点的方式来改变$w$值。你可以立即看到顶部隐藏神经元计算出的函数是如何变化的:（这里必须强烈推荐到原网站体验可视化操作界面，真的非常nice！http://neuralnetworksanddeeplearning.com/chap4.html#basic_network_precursor）
![alt text](image-6.png)
正如我们在前面学过的，被隐藏的神经元计算的是$ σ (wx + b) $[https://neuralnetworksanddeeplearning.com/chap1.html#sigmoid_neurons]，其中$ σ (z)≡\frac{1}{(1 + e^{-z})}$是sigmoid 函数。到目前为止，我们已经频繁地使用这种代数形式。但是为了证明普遍性，我们将完全忽略代数，而是操纵和观察图中所显示的形状，从而获得更多的见解。这不仅能让我们更好地了解正在发生的事情，还能证明[4]激活函数的普适性还适用于sigmoid函数以外的其他函数。

要开始进行这个证明，请尝试点击偏移值b(bias)，并向右拖动以增加它。你会看到，随着偏差的增加，图像会向左移动，但它的形状不会改变。

接下来，单击并向左拖动，以减少bias。你会看到，随着偏差的减小，图像会向右移动，但是，同样的，它的形状不会改变。

接下来，将权重w减少到2或3.你会看到，当你减小w时，曲线会变宽。您可能还需要改变b，以便将曲线保持在图像界面内。

最后，将重量增加到 w = 100以上.当你这样做时，曲线变得更陡，直到最后它看起来像一个阶跃函数。尝试调整偏置，使步长发生在 x = 0.3附近.下面的短片显示了您的结果应该是什么样子。点击播放按钮播放(或重播)视频:http://neuralnetworksanddeeplearning.com/chap4.html#basic_network_precursor

我们可以通过大幅度增加权重来简化分析，使得输出实际上是一个阶跃函数，达到一个非常好的近似值。下面我绘制了顶部隐藏神经元权重为 w = 999时的输出。请注意，此图是静态的，您不能更改参数，如权重。
![alt text](image-7.png)

实际上，使用阶跃函数比使用一般的Sigmoid函数要容易得多。原因在于在输出层我们需要将所有隐藏神经元的贡献相加。分析一堆阶跃函数的总和很容易，但当你把一堆Sigmoid形状的曲线加在一起时，推理起来就更加困难。因此，假设我们的隐藏神经元输出的是阶跃函数，会让事情变得更容易。更具体地说，我们通过将权重w固定为一个非常大的值来做到这一点，然后通过修改偏置来设置阶跃的位置。当然，把输出视为阶跃函数是一种近似，但这是一个非常好的近似，现在我们将把它当作准确的。我稍后会回来讨论这种近似的偏差对结果的影响。

在 x 的哪个值处会发生跃变？换个方式说，跃变的位置如何取决于权重和偏置？

要回答这个问题，请尝试修改上面图表中的权重和偏置（您可能需要稍微向后滚动）。您能想出阶跃位置与 w 和 b 有关的方式吗？通过一点努力，您应该能够说服自己，步骤的位置与 b 成比例，与 w 成反比。

实际上，这一步骤在位置$s =  -\frac{b}{w} $，您可以看到通过修改以下图表中的权重和偏差：（动手实践一下很直观的感受到s与b成正比，与w成反比。）
![alt text](image-8.png)

使用单个参数s来描述隐藏神经元将极大地简化我们的生活，该参数为步距$s =  -\frac{b}{w} $。尝试修改下图中的s，以适应新的参数化：
![alt text](image-9.png)

如上所述，我们已经隐式地将输入的权重 w 设置为某个较大的值——足够大，使阶跃函数成为一个很好的近似。我们可以很容易地将以这种方式参数化的神经元转换回常规模型，方法是选择偏置$ b = -ws$。


到目前为止，我们一直关注的是仅仅从顶部隐藏神经元输出。（实际上使用一个神经元的神经网络还可以简化为下图：）
![alt text](c428a9d9e8fb91bcdd2458865a915e5.jpg)
现在让我们看看整个网络的行为。特别是，我们将假设隐藏神经元正在计算由阶梯点$s_1$（顶部神经元）和$ s_2$（底部神经元）参数化的阶梯函数。它们将有各自的输出权重$w_1$和$w_2$。这是网络:
![alt text](image-10.png)

右侧绘制的是隐藏层的加权求和输出$w_1a_1+w_2a_2$。在这里，$a_1$和$a_2$分别[5]为顶部和底部隐藏神经元的输出。这些输出被标记为 a，因为它们通常被称为神经元的激活。

尝试增加和减少顶部隐藏神经元的步距 $s_1$。感受一下这如何改变隐藏层的加权输出。特别值得理解的是，当 $s_1$ 超过 $s_2$ 时会发生什么。您会发现当这种情况发生时，图形的形状会发生变化，因为我们已经从顶部隐藏神经元首先被激活的情况转变为底部隐藏神经元首先被激活的情况。

同样，尝试操纵底层隐藏神经元的步点$s_2$，并感受这如何改变隐藏神经元的联合输出。

尝试增加和减小每个输出权重。注意这如何重新缩放来自相应隐藏神经元的贡献。当其中一个权重为零时会发生什么？

最后，尝试将$w_1$设置为0.8，将$w_2$设置为-0.8。您将得到一个“突起”函数，其起点为$s_1$，终点为$s_2$，高度为0.8。例如，加权输出可能如下所示：
![alt text](image-11.png)

当然，我们可以重新调凸起到任意高度。让我们使用一个单一参数$h$，来表示高度。为了减少混乱，我还会去掉“$s_1 = …$”和“$w_1 = …$”的符号。
![alt text](image-12.png)

尝试更改h的值上下移动，看看凸起的高度如何改变。 尝试将高度更改为负值，并观察会发生什么。 尝试更改步点，看看这如何改变凸起的形状。

顺便提一下，您会注意到，我们使用神经元的方式不仅可以以图形方式来理解，还可以以更传统的编程方式来理解，比如一种if-then-else语句：

    if input >= step point:
        add 1 to the weighted output
    else:
        add 0 to the weighted output


在大多数情况下，我将坚持图形的观点。但在接下来的内容中，有时您可能会发现切换观点并考虑事物的 if-then-else 方式会有所帮助。

我们可以使用我们的凸起制作技巧，将两对隐藏神经元粘合在同一个网络中，从而获得两个凸起:
![alt text](image-14.png)
![alt text](image-13.png)

我已经压制了这里的权重，只是为每对隐藏神经元编写 h 值。尝试增加和减少两个 h 值，并观察它如何改变图形。通过更改步点来移动突起。

更一般地，我们可以使用这个想法来获得任意高度的许多峰值。特别是，我们可以将区间[0,1]分成大量N个子区间，并使用N对隐藏神经元来设置任意所需高度的峰值。让我们看看对于N = 5时如何运作。 这是相当多的神经元，所以我要稍微压缩一下。对于图表的复杂性表示歉意：我可以通过进一步概括来隐藏复杂性，但我认为应该忍受一点复杂性，以更具体地了解这些网络是如何工作的。
![alt text](image-15.png)

您可以看到隐藏神经元中有五对。各对神经元的阶梯点为0，1/5，然后是1/5，2/5，以此类推，直到4/5，5/5。这些值是固定的,它们确保我们在图表上得到五个均匀间隔的隆起。

每对神经元都有一个与之相关的值h。请记住，从神经元输出的连接具有权重h和−h（未标记）。点击其中一个h值，然后拖动鼠标向右或向左改变值。当您这样做时，观察函数的变化。通过改变输出权重，我们实际上正在设计函数！

相反，尝试单击图形，然后上下拖动以更改任何凸函数的高度。当您更改高度时，您可以看到相应的 h 值的变化。虽然没有显示，但对应的输出权重也会发生变化，分别为 +h 和 -h。

换句话说，我们可以直接操纵出现在右侧图表中的函数，并看到在左侧的 h 值中反映出来。一个有趣的事情是按住鼠标按钮并将鼠标从图表的一侧拖到另一侧。当您这样做时，您会绘制出一个函数，并观察神经网络中的参数如何适应。
![alt text](image-16.png)

是时候迎接挑战了。

让我们想想我在本章开头绘制的函数:
![alt text](image-17.png)

我当时没有说，但我绘制的实际上是这个函数:
$$f(x)=0.2+0.4x^2+0.3xsin(15x)+0.05cos(50x),(113)$$
由0绘制到1，y轴的取值范围为0到1。

这显然不是一个简单的函数。

您将学会如何使用神经网络来计算它。

在我们上面的网络中，我们一直在分析加权组合$∑_jw_ja_j$从隐藏神经元的输出。我们现在知道如何对这个数量获得很大的控制。但是，正如我之前所指出的，这个数量不是网络输出的结果。从网络输出的是$σ(∑_jw_ja_j+b)$，其中b是输出神经元上的偏置。我们是否有办法控制网络实际输出的结果？

解决方案是设计一个神经网络，其隐藏层的加权输出由$ σ^{-1}∘f(x)$ 给出，其中$σ^{-1}$只是σ函数的倒数。也就是说，我们希望从隐藏层的加权输出为：
![alt text](image-18.png)

如果我们能做到这一点，那么整个网络的输出将会很好地逼近$f(x)$[6]。

那么，你的挑战就是设计一个神经网络来近似上面所示的目标函数。为了尽可能多地学习，我要你把这道题解两次。第一次，请点击图表，直接调整不同凸点的高度功能。你应该会发现很容易找到与目标函数很好的匹配。你做得如何是通过目标函数和网络实际计算的函数之间的平均偏差来衡量的。你的挑战是尽可能地降低平均偏差。当平均偏差达到0.40或以下时，你就完成了挑战。

一旦你这样做了，点击“重置”随机重新初始化凸起。第二次解决这个问题时，要克制住点击图表的冲动。相反，修改 h数值在左手边，并再次尝试驱动平均偏差为0.40或以下。
![alt text](image-19.png)
目前平均误差大于0.64，继续调整。
![alt text](image-20.png)

现在已经算出了网络近似计算函数$f (x)$所需的所有元素$f(x)$！这只是一个粗略的近似值，但我们可以很容易地做得更好，只要增加隐藏的神经元对的数量，允许更多的凸起。

事实上，很容易将我们找到的所有数据转换回用于神经网络的标准参数化。让我快速回顾一下它是如何运作的。

第一层的权值都有一些较大的常数值，比如$ w = 1000$。

对隐藏层的神经元的偏置值是$b=-ws$。 因此，例如，对于第二个隐藏层的神经元$s=0.2$变成$b=−1000×0.2=−200。$

最后一层权重由$h$值决定。因此，例如，您在上面为第一个$h$选择的值$h=-1.2$，意味着前两个隐藏神经元的输出权重分别为$-1.2$和$1.2$,对于整个输出权重层都是同理。

最后，输出神经元的偏差为0。

这就是整个过程: 我们现在有一个神经网络的完整描述，它能很好地计算我们原来的目标函数。我们知道如何通过增加隐神经元的数量来提高近似的质量。

而且，我们最初的目标函数也没有什么特别,$f(x)=0.2+0.4x^2+0.3xsin(15x)+0.05cos(50x)$。

我们可以将此过程用于从[0,1]到[0,1]的任何连续函数。 本质上，我们正在使用单层神经网络来构建函数的查找表。 我们将能够在这个想法的基础上再接再厉，提供普遍性的一般证明。

##### 更多的输入变量
让我们将结果扩展到许多输入变量的情况。这听起来很复杂，但只要两个输入，我们需要的所有想法都可以理解。因此，让我们解决两个输入的案例。

我们将首先考虑当我们对神经元有两个输入时会发生什么：
![alt text](image-21.png)

在这里，我们有输入$x$和$y$，具有相应的权重$w_1$和$w_2$，以及神经元上的偏置$b$。 让我们把权重$w_2$设置为$0$，然后调节第一个权重，$w_1$，以及偏差$b$，看看它们如何影响神经元的输出：
![alt text](image-22.png)

如您所见，在$w_2=0$的情况下，输入$y$与神经元的输出没有区别。就好像$x$是唯一的输入。




#### 文章注解
[1]: S 函数的叠加逼近，[https://link.springer.com/article/10.1007/BF02551274]George Cybenko (1989)。这个结果在当时是非常普遍的，而且几个小组证明了密切相关的结果。Cybenko 的论文对这方面的大部分工作进行了有益的讨论。另一篇重要的早期论文是由 Kurt Hornik，Maxwell Stinchcombe 和 Halbert White (1989)提出的多层前馈网络是通用逼近器。[https://www.sciencedirect.com/science/article/abs/pii/0893608089900208]本文利用 Stone-Weierstrass 定理得到了类似的结果。
[2]实际上，计算许多函数中的一个，因为给定文本通常有许多可接受的翻译。
[3]关于翻译的备注和可能的功能更多的事项。
[4]严格来说，我所采用的视觉方法并不是传统意义上的证明。但是我相信视觉方法比传统的证明更能洞察为什么结果是真实的。当然，这种洞察力是证明背后的真正目的。有时候，在我提出的推理中会有一些小的空白: 在那些地方，我提出了一个看似合理但并不十分严谨的论点。如果这让你感到困扰，那么就把填补缺失的步骤视为一个挑战。但是不要忽视真正的目的: 理解为什么普遍性定理是正确的。
[5]顺便提一下，整个网络的输出是$σ(w_1a_1+w_2a_2+b)$，其中b是输出神经元的偏置。显然，这与隐藏层的加权输出不同，而这正是我们正在绘制的。现在我们将专注于隐藏层的加权输出，稍后再思考这与整个网络输出的关系。
[6]注意，我已经将输出神经元的偏置设置为0。




#### 日积月累
1. universality
美: [Nan]
英: [ˌju:nɪvɜ:'sælətɪ]
n.	普遍性；一般性；共相
网络:	普适性；通用性；性原则
例句：Perhaps I was trying to impress him with my precocious wisdom and the large universality of my interests.
或许，我想用我早熟的智慧与广博的兴趣给他留下深刻的印象。
2. theorem
美: [ˈθiərəm]
英: [ˈθɪərəm]
n.	（尤指数学）定理
网络：	原理；法则；定律
词形：theorems
例句：With Kuratowski's theorem, there is at least a criterion to use in discussing the nonplanarity of a graph.
有了库拉图夫斯基定理，在讨论一个图的非平面性时，至少就有了一个判别准则可供使用。

3. commonplace
美: [ˈkɑmənˌpleɪs]
英: [ˈkɒmənˌpleɪs]
n.	老生常谈；常见的事；平常的事；平淡无奇的言语等
adj.	平凡的；普通的；普遍的
v.	把…记入备忘录；由备忘录中摘出
网络:	陈腐的；平常话；凡庸
词形：commonplaces
Over the next year or two, this sort of technology is expected to become more commonplace, but it will also raise questions about privacy.
未来的一两年，我们期待这类技术变的更加普遍，当然它也带来了隐私类的问题。

4. arbitrary
美: [ˈɑrbɪˌtreri]
英: [ˈɑː(r)bɪtrəri]
adj.	任意的；武断的；随心所欲的；专横的
网络:	专断的；随意的；任意角度
This was the sort of arbitrary decision that no doubt cooled any enthusiasm he might have had for the new regime.
这种专断的决定毫无疑问扑灭了赫亚兹对新政权可能会有的热情。

5. circuit
美: [ˈsɜrkɪt]
英: [ˈsɜː(r)kɪt]
n.	回路；圈；巡回；(某一范围的)周边一圈
v.	(绕…)环行
网络:	电路；电路中；电路板
词形：circuits circuited circuiting
The standard nature of these processes permits us to engage independent silicon foundries to fabricate our integrated circuits.
这些工艺的标准性使得我们可以利用单独的硅制造厂生产集成电路。

6. caveat
美: [ˈkeɪviˌæt]
英: ['keɪvi.æt]
n.	警告；【法】中止诉讼的申请；〈美〉保护发明特许权的请求书
网络	注意事项；禁令；警告说明
词形：caveats
We also list the caveats that must be considered when deciding whether or not autophagy is an important effector mechanism of cell death.
我们还列出了那些必须被考虑的注意事项，以便确定细胞自噬是不是一种细胞死亡的重要效应机制。

7. precise
美: [prɪˈsaɪs]
英: [prɪ'saɪs]
adj.	准确的；确切的；精确的；明确的
网络：	一丝不苟的；精密；精准
A precise calculagraph is usually applied to any technical fields as athletic competition that demands accurate timing.
高精度计时器常用于体育竞赛及各种要求有较精确定时的技术领域。

8. diagram
美: [ˈdaɪəˌɡræm]
英: [ˈdaɪəɡræm]
n.	图表；图解；示意图；简图
v.	图解
网络	图形；曲线图；图示
词形：diagrams、diagramed、diagrammed、diagraming、diagramming
例句：In addition, it was sometimes easy to forget you had the default Stop Node at the end of the diagram.
此外，有时很容易忘记已将缺省停止节点放在了关系图的结尾。

9. bump
美: [bʌmp]
英: [bʌmp]
v.	撞；（尤指身体部位）碰上；颠簸行进
n.	凸块；隆起；碰撞（声）；撞击（声）
adv.	突然地；扑通一声
网络：	凹凸贴图；肿块
词形：bumps、bumping、bumped
例句：The child was dragged more than 100 feet, but survived with only a bump on his head.
这孩子被拖过了100英尺，但没什么大事，只是在头上撞了个包。

10. recap
美: [ˌriˈkæp]
英: [ˌriːˈkæp]
n.	同“recapitulation”；再生轮胎
v.	=recapitulate；翻造(轮胎)
网络	扼要重述；概括；回顾
词形：recaps、recapped、recapping
Getting a solid recap of the panel has proven difficult, so if any new info's come out that we've missed, we'll be sure to let you know.
捕捉完整充实的重述内容是有点困难的，所以如果有任何已经透露的消息我们遗漏了，我们肯定会让大家知道。
