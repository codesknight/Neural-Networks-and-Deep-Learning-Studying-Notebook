
#### 原文翻译
##### 第1章 使用神经网络识别手写数字 
##### 感知器(Perceptrons)
##### Sigmoid神经元
原文链接[http://neuralnetworksanddeeplearning.com/chap1.html#sigmoid_neurons]
原文翻译：
学习算法听起来很棒。但是我们如何为神经网络设计这样的算法呢？假设我们有一个感知器网络，我们想用来学习解决一些问题。例如，网络的输入可能是来自数字的扫描手写图像的原始像素数据。我们希望网络学习权重和偏差，以便从网络的输出正确分类的数字。为了了解学习是如何工作的，假设我们在网络的某个权重(或偏差)上做一个小小的改变。我们希望这个在权重上的微小变化只引起网络输出中相应的微小变化。正如我们将要看到的，这个特性将使学习成为可能。从图表上看，这就是我们想要的(显然这个网络太简单了，不适合做手写识别!):
![alt text](image.png)
如果一个权重（或偏差）的微小变化只引起了输出的微小变化，那么我们可以利用这一变化来修改权重和偏差，使网络行为更符合我们的意图。例如，假设网络错误地将一幅图像分类为"8"，而其实应该是"9"。我们可以找出如何微调权重和偏差，使网络更接近将图像分类为"9"。然后我们会重复这个过程，一次又一次地改变权重和偏差，以产生更好的输出。网络将会不断地进行学习。

问题是，当我们的网络包含感知器时，情况并非如此。事实上，网络中任何单个感知器的权值或偏差的一个小变化，有时会导致该感知器的输出完全翻转，比如从0到1。这种翻转可能会导致网络其他部分的行为以某种非常复杂的方式完全改变。因此，尽管你的“9”现在可能被正确分类，但网络在所有其他图像上的行为可能已经以某种难以控制的方式完全改变了。这使得我们很难看到如何逐渐修改权重和偏差，以便网络更接近所期望的性能。也许有些聪明的办法可以解决这个问题，但我们目前还不清楚如何使感知机网络学习。

我们可以通过引入一种叫做Sigmoid神经元的新型人工神经元来克服这个问题。Sigmoid神经元类似于感知器，但是经过修改，它们的重量和偏差的微小变化只会导致它们输出的微小变化。这是使Sigmoid神经元网络学习的关键事实。

好的，让我来描述一下 S 型神经元。我们将用与感知器相同的方式来描述 S 型神经元：
![alt text](image-1.png)

就像感知器一样S形神经元也有输入$x_1,x_2......$这些输入也可以采用0和1之间的任何值，而不仅仅是0或1，比如0.638...也是S神经元的有效输入。就像感知器一样，S神经元对每个输入都有权重$w_1,w_2......$和一个总体的偏移值，b，但输出不是0或1。相反，它是$σ(w⋅x+b)$。其中σ被称为Sigmoid函数[1],并被定义为：
$$σ(z)≡\frac{1}{1+e^{−z}}(3)$$
更明确地说，具有输入$x_1,x_2......$、权重$w_1,w_2......$ 和偏置 b 的 Sigmoid 神经元的输出为：
$$\frac{1}{1+e^{-(\sum_{j}^{} w_jx_j+b)}}(4)$$

初看起来，乙状结肠神经元与感知器有很大的不同。如果你还不熟悉 S形函数的代数形式，它可能看起来晦涩难懂、令人生畏。事实上，感知器和乙状结肠神经元之间有许多相似之处，S形函数的代数形式更多的是一个技术细节，而不是真正的理解障碍。

为了理解与感知器模型的相似性，假设$z≡w⋅x+b$是一个很大的正数。然后$e^{−z}≈0$，所以$σ(z)≈1$。换句话说，当$z=w⋅x+b$很趋于正无穷时，S形神经元的输出约为1，就像对于感知器一样。另一方面，假设$z=w⋅x+b$是非常小的负数,那么$e^{−z}→∞$，所以$σ(z)≈0$。所以当$z=w⋅x+b$趋于负无穷，S形神经元的行为也与感知器非常接近。只有当$w⋅x+b$规模适中时，与感知器模型的偏差很大。

σ的代数形式是什么？我们如何理解这个？实际上，σ的确切形式并不那么重要，真正重要的是在图中绘制函数的形状。这里是形状：
![alt text](image-2.png)
这个形状是阶跃函数的平滑版本。
![alt text](image-3.png)
如果σ实际上是一个阶跃函数，那么 Sigmoid 神经元将成为一个感知器，因为输出将根据$w⋅x+b$是正还是负来确定是1还是0[2]。通过使用实际的σ函数，正如上面已经暗示的那样，我们得到了一个平滑的感知器。 确实，σ函数的平滑是关键，而不是它的详细形式。 σ的平滑意味着在权重中有小的变化$Δw_j$和偏差Δb将导致神经元输出的小变化Δoutput。 实际上，微积分告诉我们Δoutput可以很好地近似为
$$Δoutput≈\sum_{j}\frac{\partial output}{\partial w_j}Δw_j+\frac{\partial output}{\partial b}Δb (5)$$
总和计算涵盖所有权重$ w_j $和$ ∂output/∂w_j $以及 $∂output/∂b$，分别表示输出与$ w_j $和 $b$ 的偏导数。如果对于部分导数不熟悉，不要惊慌！虽然上面的表达看起来复杂，包含了所有的偏导数，实际上它传达的是非常简单（而且是个好消息）：Δoutput 是对权重和偏置的变化$Δw_j$ 和 Δb 的线性函数。这种线性关系使得选择权重和偏置的微小变化以实现任意所需的输出微小变化变得很容易。因此，虽然 S 型神经元在很大程度上与感知器表现出相同的性质，但它们更容易弄清楚如何改变权重和偏置来改变输出。

如果真正重要的是 σ 的形状，而不是其确切形式，那么为什么在方程 (3)中使用了σ的特定形式？事实上，书中稍后我们偶尔会考虑输出为$f(w⋅x+b)$的神经元，对于一些其他激活函数$f(⋅)$。当我们使用不同的激活函数时，改变的主要事情是方程(5)中偏导数的特定值会改变。事实证明，当我们稍后计算这些偏导数时，使用σ将简化代数，仅仅因为指数函数在求导时具有可爱的性质。无论如何，在神经网络的工作中σ通常被使用，并且是本书中我们最经常使用的激活函数。

我们应该如何解释来自 S 型神经元的输出？显然，感知器和 S 型神经元之间的一个重要区别是，S 型神经元不仅仅输出 0 或 1。它们可以作为输出值在 0 和 1 之间的任意实数，因此诸如 0.173… 和 0.689… 这样的数值是合法的输出。当我们希望使用输出值表示输入到神经网络的图像像素的平均强度时，这可能很有用。但有时候可能会带来麻烦。假设我们希望网络的输出指示“输入图像是 9”或“输入图像不是 9”。显然，如果输出是 0 或 1，如感知器那样，这将是最容易的。但实际上我们可以建立一种惯例来处理这种情况，例如，决定将至少为 0.5 的任何输出解释为指示“9”，将小于 0.5 的任何输出解释为指示“不是 9”。我会始终明确说明我们何时使用这样的惯例，这样就不会引起任何混乱。

###### 练习

* **仿真感知器的Sigmoid神经元，第一部分**
假设我们取感知器网络中的所有权重和偏置，并将它们乘以一个正常数 c > 0。证明网络的行为不会发生变化。
* **S型神经元模拟感知器，第二部分**
假设我们有与上一个问题相同的设置——感知器网络。还假设已经选择了感知器网络的整体输入。我们不需要实际的输入值，我们只需要输入已经固定。假设权重和偏置被选择得对于网络中任何特定感知器的输入xx来说w ⋅ x + b ≠ 0。现在用S型神经元替换网络中的所有感知器，并且将权重和偏置乘以一个正的常数 c > 0。证明当c → ∞时，这个S型神经元网络的行为与感知器网络完全相同。当$w ⋅ x + b = 0$时，感知器之一发生什么问题？

##### 神经网络的架构
##### 用于对手写数字进行分类的简单神经网络
###### 练习
##### 学习梯度下降
###### 练习
###### 练习
##### 实现我们的网络来对数字进行分类
###### 练习
###### 练习
##### 走向深度学习


#### 注释说明
[1]顺便说一下 σ有时被称为 Logistic函数这种新的神经元被称为逻辑神经元。记住这些术语是很有用的，因为许多使用神经网络的人都使用这些术语。然而，我们将坚持使用Sigmoid术语。
[2]实际上，当$ w ⋅ x + b = 0 $时，感知器输出为0，而阶跃函数输出为1。因此，严格来讲，我们需要在那一点修改阶跃函数。但是你知道这个意思。

#### 日积月累
1. terrific
美: [təˈrɪfɪk]
英: [tə'rɪfɪk]
adj.	极好的；绝妙的；了不起的；很大的
网络：	可怕的；极大的；好极了
例句："gradually, " the pig "drum belly up, he was always smiling, looked at me as if to say: " You're terrific!
渐渐的，“小猪”的肚子鼓了起来，他总是笑眯眯地看着我，好像在说：“你真棒！”