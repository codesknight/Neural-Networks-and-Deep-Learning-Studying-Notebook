
#### 原文翻译
##### 第1章 使用神经网络识别手写数字 
人类视觉系统是世界奇迹之一。考虑以下手写数字序列：
![alt text](image-4.png)

大多数人毫不费力地将这些数字识别为 504192。这种轻松具有欺骗性。在我们大脑的每个半球中，人类都有一个初级视觉皮层，也称为 V1，包含 1.4 亿个神经元，它们之间有数百亿个连接。然而，人类视觉不仅涉及 V1，还涉及整个视觉皮层系列 - V2、V3、V4 和 V5 - 进行逐渐复杂的图像处理。我们的大脑中装有一台超级计算机，经过数亿年的进化调整，非常适合理解视觉世界。识别手写数字并不容易。相反，我们人类非常擅长理解我们眼睛所看到的东西。但几乎所有这些工作都是在不知不觉中完成的。因此，我们通常不会意识到我们的视觉系统解决的问题有多么困难。

如果您尝试编写计算机程序来识别上述数字，那么视觉模式识别的难度就会变得显而易见。当我们自己做的时候，看似容易的事情突然变得极其困难。关于我们如何识别形状的简单直觉——“9 的顶部有一个圈，右下角有一个竖线”——结果用算法来表达并不那么简单。当您试图使这些规则变得精确时，您很快就会迷失在例外、警告和特殊情况的泥潭中。看来是无望了。

神经网络以不同的方式解决这个问题。这个想法是采用大量手写数字，称为训练示例，
![alt text](image-5.png)

然后开发一个可以从这些训练示例中学习的系统。换句话说，神经网络使用示例来自动推断识别手写数字的规则。此外，通过增加训练示例的数量，网络可以更多地了解手写内容，从而提高其准确性。因此，虽然我上面只展示了 100 个训练数字，但也许我们可以通过使用数千甚至数百万或数十亿个训练示例来构建更好的手写识别器。

在本章中，我们将编写一个计算机程序来实现一个学习识别手写数字的神经网络。该程序只有 74 行长，并且没有使用特殊的神经网络库。但这个短程序可以识别数字，准确率超过 96%，无需人工干预。此外，在后面的章节中，我们将提出可以将准确率提高到 99% 以上的想法。事实上，最好的商业神经网络现在非常好，银行用它们来处理支票，邮局用它们来识别地址。

我们专注于手写识别，因为它是学习一般神经网络的一个很好的原型问题。作为一个原型，它达到了最佳点：它具有挑战性 - 识别手写数字不是一件小事 - 但它并不难到需要极其复杂的解决方案或巨大的计算能力。此外，这是开发更先进技术（例如深度学习）的好方法。因此，在整本书中，我们将反复讨论手写识别问题。在本书的后面，我们将讨论如何将这些想法应用于计算机视觉以及语音、自然语言处理和其他领域的其他问题。

当然，如果本章的重点只是编写一个计算机程序来识别手写数字，那么这一章就会短得多！但在此过程中，我们将发展许多关于神经网络的关键思想，包括两种重要类型的人工神经元（感知器和 S 型神经元），以及神经网络的标准学习算法（称为随机梯度下降）。在整个过程中，我重点解释为什么事情是这样完成的，并建立你的神经网络直觉。这需要比我只介绍正在发生的事情的基本机制更长的讨论，但为了您获得更深入的理解，这是值得的。在收获中，到本章结束时，我们将能够理解深度学习是什么以及它为何重要。


##### 感知器(Perceptrons)
什么是神经网络？首先，我将解释一种称为感知器的人工神经元。感知器是由科学家 Frank Rosenblatt 在 20 世纪 50 年代和 1960 年代开发的，其灵感来自 Warren McCulloch 和 Walter Pitts 的早期工作。如今，使用其他人工神经元模型更为常见 - 在本书以及许多现代神经网络研究中，使用的主要神经元模型是 S 型神经元。我们很快就会讨论乙状结肠神经元。但要理解为什么 S 型神经元是这样定义的，值得花时间首先了解感知器。

那么感知器是如何工作的呢？感知器接受多个二进制输入$ x_1,x_2,… $，并生成单个二进制输出：
![alt text](image-6.png)

在所示的示例中，感知器有三个输入。$x_1,x_2,x_3$。一般情况下，它可能具有更多或更少的输入。Rosenblatt提出了一个简单的规则来计算输出。他介绍了权重$w_1,w_2……$表示输入对输出的重要性。然后，神经元的输出值为 0 或 1，取决于加权和$ ∑_j w_j x_j $是否小于或大于某个阈值。与权重一样，阈值是神经元的一个参数，它是一个实数。更精确地说，可以表述为：
\[
\text{output} = 
\begin{cases} 
0 & \text{if } \sum_{j} w_{j}x_{j} \leq \text{threshold} \\
1 & \text{if } \sum_{j} w_{j}x_{j} > \text{threshold}
\end{cases}(1)
\]
感知器是如何工作的，就是这些内容了！

这就是基本的数学模型。您可以这样理解感知器：它是一种通过权衡证据做出决策的设备。让我举个例子。这不是一个非常现实的例子，但是很容易理解，我们很快就会看到更现实的例子。假设周末即将到来，您听说您所在的城市将举办奶酪节。您喜欢奶酪，正在考虑是否参加这个节日。您可以通过权衡三个因素来做出决定：

1. 天气好吗？
2. 你的男朋友或女朋友想陪你吗？
3. 音乐节附近有公共交通吗？ （您没有汽车）。

我们可以用相应的二进制变量$ x_1、x_2$ 和 $x_3 $来表示这三个因素。例如，如果天气很好，我们会有 $x_1 = 1$，如果天气不好，就有$ x_1 = 0$。同样，如果你的男朋友或女朋友想去，$x_2 = 1$，否则为$ x_2 = 0$。对于 $x_3 $和公共交通也是类似的。

现在，假设您非常喜欢奶酪，以至于即使您的男朋友或女朋友不感兴趣并且很难去参加节日，您也很乐意去参加节日。但也许你真的很讨厌坏天气，如果天气不好你就不可能去参加音乐节。您可以使用感知器来模拟此类决策。

一种方法是为天气选择权重 $w_1=6$ ，为其他条件选择 $w_2=2$ 和 $w_3=2$ 。 $w_1$ 的值越大，表明天气对您来说越重要，远比您的男朋友或女朋友是否加入您或公共交通的远近重要得多。最后，假设您为感知器选择阈值 5 。通过这些选择，感知器实现所需的决策模型，每当天气好的时候输出 1 ，每当天气不好的时候输出 0 。无论你的男朋友或女朋友是否想去，或者附近是否有公共交通，这对输出都没有影响。

通过改变权重和阈值，我们可以获得不同的决策模型。例如，假设我们选择了阈值 3 。然后，感知器会决定，只要天气好，或者节日靠近公共交通并且你的男朋友或女朋友愿意加入你，你就应该去参加节日。换句话说，这将是一种不同的决策模式。降低门槛意味着你更愿意去参加节日。

显然，感知器并不是人类决策的完整模型！但这个例子说明了感知器如何权衡不同类型的证据以做出决策。复杂的感知器网络可以做出相当微妙的决定，这似乎是合理的：
![alt text](image-8.png)

在这个网络中，感知器的第一列 - 我们称之为感知器的第一层 - 通过权衡输入证据做出了三个非常简单的决定。那么第二层的感知器呢？这些感知器中的每一个都是通过权衡来自第一层决策的结果来做出决定的。这种方式，第二层中的感知器可以在比第一层感知器更复杂和更抽象的水平上做出决策。甚至更复杂的决策可以由第三层的感知器做出。这样，一个由多层感知器组成的网络可以进行复杂的决策。

顺便说一句，当我定义感知器时，我说过感知器只有一个输出。在上面的网络中，感知器看起来有多个输出。事实上，它们仍然是单输出。多个输出箭头只是指示感知器的输出被用作几个其他感知器的输入的有用方式。它比绘制一条然后分开的输出线更方便。

让我们简化描述感知器的方式。条件 $∑_jw_jx_j>threshold$ 很麻烦，我们可以做两个符号上的改变来简化它。第一个更改是将$ ∑_jw_jx_j $写为点积 $w⋅x≡∑_jw_jx_j$ ，其中 $w$ 和 $x$ 是向量，其分量是权重和分别输入。第二个更改是将阈值移至不等式的另一侧，并将其替换为感知器偏差$ b≡−threshold$ 。使用偏差而不是阈值，可以重写感知器规则：
\[
\text{output} = 
\begin{cases} 
0 & \text{if } w \cdot x + b \leq 0 \\
1 & \text{if } w \cdot x + b > 0
\end{cases}(2)
\]

您可以将偏差视为衡量感知器输出 1 难易程度的指标。或者用更生物学的术语来说，偏差是衡量感知器启动的难易程度的指标。对于具有很大偏差的感知器，感知器非常容易输出 1 。但如果偏差非常负，那么感知器就很难输出 1 。显然，引入偏差只是我们描述感知器方式的一个小变化，但我们稍后会看到它会导致符号的进一步简化。因此，在本书的其余部分中，我们不会使用阈值，我们将始终使用偏差。

我将感知器描述为一种权衡证据以做出决策的方法。感知器的另一种使用方式是计算我们通常认为是底层计算的基本逻辑函数，例如 AND 、 OR 和 NAND 等函数。例如，假设我们有一个具有两个输入的感知器，每个输入都有权重 −2 ，总体偏差为 3 。这是我们的感知器：
![alt text](image-9.png)

然后我们看到，输入00 产生输出1，因为$(−2) * 0 + (−2) * 0 + 3 = 3 $是正的。 在这里，我引入了* 符号，以使乘法显示出来。 类似的计算显示，输入01 和10 产生输出1。 但输入11 产生输出0，因为$(−2) * 1 + (−2) * 1 + 3 = −1 $是负的。 因此，我们的感知器实现了一个NAND门！

NAND示例表明我们可以使用感知器来计算简单的逻辑函数。实际上，我们可以使用感知器网络来计算任何逻辑函数。原因在于NAND门对于计算是通用的，也就是说，我们可以用NAND门构建任何计算。例如，我们可以使用NAND门构建一个添加两个位x₁和x₂的电路。这需要计算位求和，x₁⊕x₂，以及一个进位位，当x₁和x₂都为1时设为1，即进位位只是位乘积x₁x₂：
![alt text](image-10.png)

为了获得一个与感知机等效的网络，我们将所有的NAND门替换为两个输入的感知机，每个输入的权重为-2，总体偏差为3。这就是结果网络。请注意，我将底部右侧的NAND门对应的感知机稍微移动了一下，只是为了更容易地在图表上画出箭头：
![alt text](image-11.png)

感知机网络中的一个显著特点是，最左侧感知机的输出被用作最底端感知机的两倍输入。当我定义感知机模型时，我没有说这种双输出到同一位置是否被允许。实际上，这并不太重要。如果我们不想允许这种情况，那么可以简单地合并这两条线，将它们合并成一条连接，权重为-4，而不是两条权重为-2的连接。（如果你觉得这显而易见，请停下来证明这是等价的。）通过这种改变，网络如下所示，所有未标记的权重都等于-2，所有偏差都等于3，并有一条标记为-4的单一权重：
![alt text](image-12.png)

到目前为止，我一直将像x1和x2这样的输入画成浮动在感知器网络左侧的变量。 实际上，画一个额外的感知器层 - 输入层 - 用于编码输入是常规做法：
![alt text](image-13.png)

这是关于输入感知器的符号表示法，其中我们有一个输出，但没有输入。
![alt text](image-14.png)

这是一个简称。它实际上并不表示没有输入的感知器。要看到这一点，假设我们有一个没有输入的感知器。那么加权和$∑_jw_jx_j$将永远为零，因此如果$b > 0$，感知器将输出1，如果$b ≤ 0$，则输出0。也就是说，感知器将简单地输出一个固定值，而不是期望的值（如上面的$x_1$）。最好将输入感知器视为根本不是感知器，而是简单地被定义为输出期望的值$x_1，x_2，…$。

加法器示例演示了如何使用感知器网络来模拟包含多个NAND门的电路。由于NAND门是通用计算的，因此感知器也是通用计算的。

感知器的计算普适性既令人 gerong, 又令人失望。令人 gerong， 因为它告诉我们，感知器网络可以和任何其他计算设备一样强大。但是，其拙幼几乎像是一种新型 NAND 门。这几乎不是什么大新闻！

然而，情况比这种观点所暗示的要好。 原来我们可以设计学习算法，可以自动调整人工神经元网络的权重和偏差。 这种调整是针对外部刺激发生的，不需要程序员直接介入。 这些学习算法使我们能够以一种根本不同于传统逻辑门的方式使用人工神经元。 我们的神经网络不是明确地列出 NAND 和其他门的电路，而是简单地学会解决问题，有时是一些极其难以直接设计传统电路的问题。


##### Sigmoid神经元
学习算法听起来很棒。但是我们如何为神经网络设计这样的算法呢？假设我们有一个感知器网络，我们想用来学习解决一些问题。例如，网络的输入可能是来自数字的扫描手写图像的原始像素数据。我们希望网络学习权重和偏差，以便从网络的输出正确分类的数字。为了了解学习是如何工作的，假设我们在网络的某个权重(或偏差)上做一个小小的改变。我们希望这个在权重上的微小变化只引起网络输出中相应的微小变化。正如我们将要看到的，这个特性将使学习成为可能。从图表上看，这就是我们想要的(显然这个网络太简单了，不适合做手写识别!):
![alt text](image.png)
如果一个权重（或偏差）的微小变化只引起了输出的微小变化，那么我们可以利用这一变化来修改权重和偏差，使网络行为更符合我们的意图。例如，假设网络错误地将一幅图像分类为"8"，而其实应该是"9"。我们可以找出如何微调权重和偏差，使网络更接近将图像分类为"9"。然后我们会重复这个过程，一次又一次地改变权重和偏差，以产生更好的输出。网络将会不断地进行学习。

问题是，当我们的网络包含感知器时，情况并非如此。事实上，网络中任何单个感知器的权值或偏差的一个小变化，有时会导致该感知器的输出完全翻转，比如从0到1。这种翻转可能会导致网络其他部分的行为以某种非常复杂的方式完全改变。因此，尽管你的“9”现在可能被正确分类，但网络在所有其他图像上的行为可能已经以某种难以控制的方式完全改变了。这使得我们很难看到如何逐渐修改权重和偏差，以便网络更接近所期望的性能。也许有些聪明的办法可以解决这个问题，但我们目前还不清楚如何使感知机网络学习。

我们可以通过引入一种叫做Sigmoid神经元的新型人工神经元来克服这个问题。Sigmoid神经元类似于感知器，但是经过修改，它们的重量和偏差的微小变化只会导致它们输出的微小变化。这是使Sigmoid神经元网络学习的关键事实。

好的，让我来描述一下 S 型神经元。我们将用与感知器相同的方式来描述 S 型神经元：
![alt text](image-1.png)

就像感知器一样S形神经元也有输入$x_1,x_2......$这些输入也可以采用0和1之间的任何值，而不仅仅是0或1，比如0.638...也是S神经元的有效输入。就像感知器一样，S神经元对每个输入都有权重$w_1,w_2......$和一个总体的偏移值，b，但输出不是0或1。相反，它是$σ(w⋅x+b)$。其中σ被称为Sigmoid函数[1],并被定义为：
$$σ(z)≡\frac{1}{1+e^{−z}}(3)$$
更明确地说，具有输入$x_1,x_2......$、权重$w_1,w_2......$ 和偏置 b 的 Sigmoid 神经元的输出为：
$$\frac{1}{1+e^{-(\sum_{j}^{} w_jx_j+b)}}(4)$$

初看起来，乙状结肠神经元与感知器有很大的不同。如果你还不熟悉 S形函数的代数形式，它可能看起来晦涩难懂、令人生畏。事实上，感知器和乙状结肠神经元之间有许多相似之处，S形函数的代数形式更多的是一个技术细节，而不是真正的理解障碍。

为了理解与感知器模型的相似性，假设$z≡w⋅x+b$是一个很大的正数。然后$e^{−z}≈0$，所以$σ(z)≈1$。换句话说，当$z=w⋅x+b$很趋于正无穷时，S形神经元的输出约为1，就像对于感知器一样。另一方面，假设$z=w⋅x+b$是非常小的负数,那么$e^{−z}→∞$，所以$σ(z)≈0$。所以当$z=w⋅x+b$趋于负无穷，S形神经元的行为也与感知器非常接近。只有当$w⋅x+b$规模适中时，与感知器模型的偏差很大。

σ的代数形式是什么？我们如何理解这个？实际上，σ的确切形式并不那么重要，真正重要的是在图中绘制函数的形状。这里是形状：
![alt text](image-2.png)
这个形状是阶跃函数的平滑版本。
![alt text](image-3.png)
如果σ实际上是一个阶跃函数，那么 Sigmoid 神经元将成为一个感知器，因为输出将根据$w⋅x+b$是正还是负来确定是1还是0[2]。通过使用实际的σ函数，正如上面已经暗示的那样，我们得到了一个平滑的感知器。 确实，σ函数的平滑是关键，而不是它的详细形式。 σ的平滑意味着在权重中有小的变化$Δw_j$和偏差Δb将导致神经元输出的小变化Δoutput。 实际上，微积分告诉我们Δoutput可以很好地近似为
$$Δoutput≈\sum_{j}\frac{\partial output}{\partial w_j}Δw_j+\frac{\partial output}{\partial b}Δb (5)$$
总和计算涵盖所有权重$ w_j $和$ ∂output/∂w_j $以及 $∂output/∂b$，分别表示输出与$ w_j $和 $b$ 的偏导数。如果对于部分导数不熟悉，不要惊慌！虽然上面的表达看起来复杂，包含了所有的偏导数，实际上它传达的是非常简单（而且是个好消息）：Δoutput 是对权重和偏置的变化$Δw_j$ 和 Δb 的线性函数。这种线性关系使得选择权重和偏置的微小变化以实现任意所需的输出微小变化变得很容易。因此，虽然 S 型神经元在很大程度上与感知器表现出相同的性质，但它们更容易弄清楚如何改变权重和偏置来改变输出。

如果真正重要的是 σ 的形状，而不是其确切形式，那么为什么在方程 (3)中使用了σ的特定形式？事实上，书中稍后我们偶尔会考虑输出为$f(w⋅x+b)$的神经元，对于一些其他激活函数$f(⋅)$。当我们使用不同的激活函数时，改变的主要事情是方程(5)中偏导数的特定值会改变。事实证明，当我们稍后计算这些偏导数时，使用σ将简化代数，仅仅因为指数函数在求导时具有可爱的性质。无论如何，在神经网络的工作中σ通常被使用，并且是本书中我们最经常使用的激活函数。

我们应该如何解释来自 S 型神经元的输出？显然，感知器和 S 型神经元之间的一个重要区别是，S 型神经元不仅仅输出 0 或 1。它们可以作为输出值在 0 和 1 之间的任意实数，因此诸如 0.173… 和 0.689… 这样的数值是合法的输出。当我们希望使用输出值表示输入到神经网络的图像像素的平均强度时，这可能很有用。但有时候可能会带来麻烦。假设我们希望网络的输出指示“输入图像是 9”或“输入图像不是 9”。显然，如果输出是 0 或 1，如感知器那样，这将是最容易的。但实际上我们可以建立一种惯例来处理这种情况，例如，决定将至少为 0.5 的任何输出解释为指示“9”，将小于 0.5 的任何输出解释为指示“不是 9”。我会始终明确说明我们何时使用这样的惯例，这样就不会引起任何混乱。

###### 练习
* **仿真感知器的Sigmoid神经元，第一部分**
假设我们取感知器网络中的所有权重和偏置，并将它们乘以一个正常数 c > 0。证明网络的行为不会发生变化。
* **S型神经元模拟感知器，第二部分**
假设我们有与上一个问题相同的设置——感知器网络。还假设已经选择了感知器网络的整体输入。我们不需要实际的输入值，我们只需要输入已经固定。假设权重和偏置被选择得对于网络中任何特定感知器的输入xx来说w ⋅ x + b ≠ 0。现在用S型神经元替换网络中的所有感知器，并且将权重和偏置乘以一个正的常数 c > 0。证明当c → ∞时，这个S型神经元网络的行为与感知器网络完全相同。当$w ⋅ x + b = 0$时，感知器之一发生什么问题？

##### 神经网络的架构
##### 用于对手写数字进行分类的简单神经网络
###### 练习
##### 学习梯度下降
###### 练习
###### 练习
##### 实现我们的网络来对数字进行分类
###### 练习
###### 练习
##### 走向深度学习


#### 注释说明
[1]顺便说一下 σ有时被称为 Logistic函数这种新的神经元被称为逻辑神经元。记住这些术语是很有用的，因为许多使用神经网络的人都使用这些术语。然而，我们将坚持使用Sigmoid术语。
[2]实际上，当$ w ⋅ x + b = 0 $时，感知器输出为0，而阶跃函数输出为1。因此，严格来讲，我们需要在那一点修改阶跃函数。但是你知道这个意思。

#### 日积月累
1. terrific
美: [təˈrɪfɪk]
英: [tə'rɪfɪk]
adj.	极好的；绝妙的；了不起的；很大的
网络：	可怕的；极大的；好极了
例句："gradually, " the pig "drum belly up, he was always smiling, looked at me as if to say: " You're terrific!
渐渐的，“小猪”的肚子鼓了起来，他总是笑眯眯地看着我，好像在说：“你真棒！”